---
layout: post
title: "机器学习 · 监督学习篇 IV"
subtitle: "K近邻"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 机器学习
  - 机器学习·监督学习篇
---

### 一、概念

K最近邻(k-Nearest Neighbor，KNN)分类算法，一种很 “直白” 的算法，它的算法思路是，如果一个样本在特征空间中 K 个最相似（度量距离最近）的样本中大多数属于某一类别，那么这个样本也属于这个类别，以下图的情况为例，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/knn_1.jpg"/>
</center>

，如上图的情况，如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形；
如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。我们可以看到，
 KNN 本质是基于一种数据统计的方法。

KNN 是一种 memory-based learning，也叫 instance-based learning，没有显式的前期训练过程，而是在预测时把数据集加载到内存后，根据
数据样本来进行分类。个人认为朴素贝叶斯模型与 KNN 在这一点上很相似，也是没有显式的训练过程。

**KNN 与 K-Means 的区别**

| | KNN | K-Means |
| :-----------:| :----------: |:----------: | 
| 类型 | 监督学习、分类 | 无监督学习、聚类 |
| 训练过程 | 无显式 | 显式 |
| k的含义 | 预测阶段分类参考的样本数目 | 训练阶段聚类的类别数 |

**KNN 与 K-Means 的共性**

虽然用途不一样，但都包含了一个给定一个点，在数据集中找到离它最近的点，即两者都用到了近邻算法，如果寻求高效的话一般用 KD 树来实现近邻算法。

### 二、算法步骤

根据实现方式的不同，将 KNN 分为 “朴素实现” 和 “KD树实现”，假定训练数据量为 N。

**2.1 朴素实现**

1. 计算测试数据与各个训练数据之间的距离；复杂度为 O(N)；
2. 选取距离最小的 K 个点；利用最大K-堆排序，一般认为复杂度为 O(NlogK)，然而实际平均复杂度为 O(N + KlogK)，这个需要推导一下；
3. 确定这 K 个带你所在类别的出现频率，选择频率最高的分类作为测试数据的类别；复杂度为 O(K)；

所以最后，由于没有训练过程，故训练复杂度是 O(1)，测试复杂度是 O(N + KlogK)。

**2.2 KD树实现**

1. 为训练数据建立 KD 树；复杂度为 O(NlogN)；
2. 选取距离最小的 K 个点，这里的 K 与 KD树 的 K 含义不同，KD树 中的 K 表示的是数据维度；利用 KD树 的搜索，平均复杂度为 O(logN)，当然，最坏复杂度会比较高；
3. 最后一步与朴素实现一样，确定这 K 个带你所在类别的出现频率，选择频率最高的分类作为测试数据的类别；复杂度为 O(K)；

所以最后，训练复杂度是 O(NlogN)，测试复杂度是 O(KlogN)，可以看到，当 k << N 时，测试性能比朴素实现高很多。

### 三、代码实现



[Kmeans算法与KNN算法的区别](https://www.cnblogs.com/peizhe123/p/4619066.html)