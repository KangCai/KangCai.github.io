---
layout: post
title: "游戏AI参考 II"
subtitle: "DOTA2 5v5 AI - OpenAI Five"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 游戏AI
---

> 为了研究高水平游戏AI的开发，利用业余时间里观摩一下目前国际上最高水平游戏AI是怎么做的。本文的内容是2018年上半年颇受关注的 OpenAI 团队做的 DOTA2 5v5 AI，名为 OpenAI Five。OpenAI 团队在2017年年中做的 DOTA2 影魔 solo AI 能够完胜职业选手，而2018年的 AI 也达到了平均6500天梯分（最近情况不清楚了，大概世界前1000名吧），总之水平很高。本文期望通过对它进行研究，尝试发掘一些可供 AI 开发参考的有效信息。

> 文章首发于[我的博客](https://kangcai.github.io/)，转载请保留链接 ;)

**我的收获（一些重要信息）**

1. OpenAI Five 从随机权重开始训练，为了避免 “策略崩溃”，**80%的游戏与自己战斗，20%的游戏与过去的自己战斗，来进行强化学习迭代**；
2. Delay Reward 的问题是通过**最大化未来 reward 的指数衰减总和**来做的；
3. AI 的团队性做单独的团队策略机制，而是通过**团队平均 reward 的形式来影响个体 AI 的决策**；
4. **通过超参数来决定 AI 是更在乎个体 reward 还是团队平均 reward**，博客原文里有这么一句 “We anneal its value from 0 to 1 over training” ，看起来在训练的时候都尝试过，选一个表现最好的超参数。有一个疑问，这个超参数看起来应该在一场比赛中发生变化才合理吧，比如前期更重视个体 reward，后期更重视团队 reward，不知道这句原文是不是这个意思；
5. 原文说 DOTA2 游戏环境一次 tick 花费几毫秒，而 OpenAI Five又是**每4帧获取一个样本，意味着实际运行时每10毫秒~100毫秒获取一次样本**，具体不清楚；
6. 原文说每天相当于打了180年，故可以看出同一时间大约**并行跑了6480场比赛**；
7. 使用**PPO算法**进行强化学习，其中**Actor网络结构为：输入网络 + 共享网络 + 输出网络。其中，输入网络是层次式的 各种State（约20000维度） + FC-relu + concat + max-pool 的结构，共享网络是经过1024个节点的 LSTM，最后的输出分为 操作、技能偏移X、技能偏移Y、移动目标点X、移动目标点Y、传送目标、操作延迟帧数、选择目标 共8大项，每一项都是一个 FC + Softmax 的分类（不连续）输出网络**；
8. 输入特征中，**位置信息用的是绝对位置**，还用到了**单位类型，动作，当然两个都是Embedding**的，**单位状态**当然也必须有，但有三个特征值得特别注意下：**正在攻击的敌方和正被敌方或友方英雄攻击的信息（被友方英雄攻击应该是反补）、最近12帧的血量信息（应该是指短时间内的掉血情况吧）、距离所有友方和敌方的距离#**；
9. 输入网络中，**不定数量的单位状态的输入处理，是通过 FC 之后的 max-pool 来合并的，表明所有单位使用的是同一套网络和参数**，而 max-pool 每次都针对性的 BP 迭代 max-pool 中 max 位置对应输出的上一层单位网络；
10. 输出网络中，**操作输出网络在 FC 层输出向量点乘了当前可使用操作的热编码向量，选敌输出网络在 FC 输出向量点乘了单位的attantion keys**；
11. OpenAI的**APM是 150-170**；
12. **二元奖励效果更好**。不仅仅包含最后的胜利，如果还包含了中间的小奖励，训练更加平稳，效果更好；
13. **没有通过分层强化学习来做，而是直接有5分钟的半衰期的 reward 衰减系数来实现长时间预期**；
14. **技能加点和物品购买是脚本写的**；
15. 我推测1天大约跑23万场比赛，共6天，故**总训练场次约140万场**。

Actor 网络模型如下图1所示，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/OpenAI Five Model.jpg"/>
</center>
<center>图1 OpenAI Five 网络模型</center>