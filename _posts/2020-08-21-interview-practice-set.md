---
layout: post
title: "面试 · 实战合集"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 面试
---

## 一、定范围
【先定个范围，判断是否既了解深度学习，又了解传统机器学习，还是只了解一方面】

**问：除了深度学习模型外，还了解其它传统机器学习模型吗？ or 除了传统机器学习之外，还了解深度学习吗？ or 除了 朴素贝叶斯（举个例子，比如是面试者简历里写到的）还了解其他机器学习模型吗？**

答：了解传统机器学习，则可以问传统机器学习问题；了解深度学习，则可以问深度学习问题。

## 二、传统机器学习基础问题
【传统机器学习基础问题，基本要求都懂的知识】

**问：机器学习判别模型（Discriminative Model）和生成模型（Generative Model）的区别？**

答：判别模型对判别标准直接建模，如下所示的图中的线一样判别（划分）数据。

<img src="https://kangcai.github.io/img/in-post/post-interview/i1.png"/>

生成模型在类内对样本的联合概率分布的建模，以此为基础再做分类，如下所示，

<img src="https://kangcai.github.io/img/in-post/post-interview/i2.png"/>


**问：判别模型有哪些，生成模型有哪些，or逻辑回归是什么类型的模型，支持向量机是什么类型的模型，朴素贝叶斯是什么类型的模型？**

答：常见判别模型有 逻辑回归（或者叫逻辑斯谛克回归，Logistic Regression，LR）、支持向量机（Support Vector Machine, SVM）、神经网络（Neural network ，NN）、感知机（Perceptron）、K近邻（K-nearest neighbor，KNN）、决策树（Decision Tree，DT）、随机森林（Random Forest）、梯度提升树（GBDT）、最大熵模型（Maximum Entropy，ME），前面几项按理说都应该知道，线性判别分析（Linear Discriminant Analysis，LDA）、Boosting、条件随机场（CRF）模型，后面可能会不了解；常见生成模型有 朴素贝叶斯（NB）、隐马尔科夫模型（HMM），前面两个应该知道，高斯判别分析（Gaussian Discriminant Analysis，GDA）、隐含狄利克雷分布模型（Latent Dirichlet Allocation，LDA，用于文档主题生成模型，NLP自然语言处理方向的同学应该都知道）、受限玻尔兹曼机（RBM），后面可能会不了解。

**问：逻辑回归模型（也叫罗吉斯蒂克回归）是回归模型还是分类模型？**

答：只能是分类模型（回答说回归模型基本就可以判定对传统机器学习不了解）。

**问：如何判断模型是否过拟合？如何判断模型是否欠拟合？**

答：模型在训练集上表现很好，但在测试集和新数据上的表现较差，表示模型是过拟合；模型在训练和预测时表现都不好，表示模型是欠拟合。

**问：降低过拟合风险的方法有哪些？降低欠拟合风险的方法有哪些？**

答：降低过拟合的方法：增加训练数据集、降低模型复杂度、正则化（L2正则最常用）、集成学习（多个模型集成在一起，如Bagging方法）。
降低欠拟合风险的办法：添加新特征、增加模型复杂度、减小正则化系数。

**问：为什么需要对数值类型的特征做归一化？**

答：因为不同维度上的特征如果取值范围差别很大，会导致对应的参数在更新的时候更新速度差别较大：以梯度下降法为例，不同维度上的特征如果取值范围差别很大会导致需要较多的迭代次数才能找到最优解，归一化之后不同维度特征更新速度变得一致，容易更快地通过梯度下降找到最优解，如下图所示

<img src="https://kangcai.github.io/img/in-post/post-interview/i3.jpg"/>

**问：说说你对L0正则化、L1正则化和L2正则化的理解**

答：正则化即Regularization，是对 y=wx+b（分类任务中的判别函数，回归问题中的拟合函数）中参数 w 的约束，在目标函数中体现就是关于 w 的函数项。

L0范数是向量中非0的元素的个数。如果我们用L0范数来正则化一个参数向量w的话，就是希望w的大部分元素都是0。换句话说，让参数w是稀疏的。但不幸的是，L0范数的最优化问题是一个NP hard问题；而且理论上有证明，L1范数是L0范数的最优凸近似，因此通常使用L1范数来代替。

L1正则化采用L1范数对参数进行约束，L1范数是向量中各个元素绝对值之和，也称为叫“稀疏规则算子”（Lasso regularization）。L1正则化之所以可以防止过拟合，就是因为大的L1范数值表明模型不够好，具体分析如下：L1范数是各个参数的绝对值相加得到的，参数值大小和模型复杂度是成正比的，因此复杂的模型，其L1范数就大，导致损失函数大，说明复杂模型就不够好，这样在一定程度防止过拟合现象。另外，如上所说L1范数是L0范数的最优凸近似，它会使尽可能多的模型参数值为0，使参数具有稀疏性。

L2正则化采用L2范数对参数进行约束， L2范数是向量中各个元素的平方和，也叫“岭回归”（Ridge Regression）。与L1范数不一样的是，它不是使尽可能多的参数为0，而只是接近于0。越小的参数说明模型越简单，从而越不容易产生过拟合现象，通常防止模型过拟合加入的正则项就是L2正则项。
	
**问：如何处理类别型特征，比如学习成绩的优良中差，再比如血型A,B,AB,O型**

答：对于类别间具有大小关系的数据，通常使用序号编码，比如优用4表示，良用3表示，中用2表示，差用1表示；对于类别间不具有大小关系的数据，通常用热编码（独热编码，比如one-hot encoding）特征，比如A型血用（1,0,0,0）表示，B型血用（0,1,0,0）表示在，依次类推。

**问：简单说一下你对经验结构风险最小化的理解。**

答：经验风险是模型关于训练样本集的平均损失，衡量模型对样本数据集的拟合程度，单纯依据经验风险最小化（empirical risk minimization，ERM）学习的效果就未必很好，会产生过拟合现象。而结构风险最小化（structural risk minimization, SRM）是为了防止过拟合而提出的策略，结构风险最小化等价于正则化。通常模型会同时兼顾经验风险和结构风险的最小化，如下图是一种典型的目标函数形式，

<img src="https://kangcai.github.io/img/in-post/post-interview/i4.png"/>

**问：核函数（Kernel Function）是支持向量机能够应用在非线性数据的关键，那么核函数可以用在逻辑回归模型上吗？核函数的使用条件或者前提是什么？**

答：可以用在逻辑回归模型上，有些同学会以为只有SVM能用，其实是不对的；
核函数使用条件是损失函数公式中出现两个样本特征 x_i，x_j 相乘的结构，一般只有能将原问题转化成拉格朗日对偶问题的时候可以用。

**问：逻辑回归模型的损失函数是什么？**

答：交叉熵，或者写出下面式子，其中 x是特征向量，y是标签，h(x) 是sigmoid函数

<img src="https://kangcai.github.io/img/in-post/post-interview/i3.png"/>

**问：逻辑回归模型的交叉熵损失函数是依据什么假设推导出来的 or 使用逻辑回归模型的先验假设是什么？**

答：样本独立 或者 因变量（即y值，就是标签）服从伯努利分布（Bernoulli distribution），如果面试者只答服从伯努利分布，要问出哪个量服从该分布 – 因变量（标签y）；

**问：逻辑回归模型的目标函数是凸函数还是非凸函数 or 交叉熵损失函数是凸函数还是非凸函数？**

答：凸函数

**问：为求解逻辑回归模型的损失函数，一般采用哪些迭代算法 or 逻辑回归模型的目标函数求解算法通常有哪些?**

答：梯度下降法（Gradient Descent，或者有的人可能直接说 SGD 或 GD）、拟牛顿法（Quasi-Newton Methods），这两个应该要答；迭代尺度法（Iterative Scaling），这个无所谓

**问：说一下支持向量机的原理**

答：关键词 – 样本到分类超平面的最小间隔最大、原始问题的拉格朗日函数、KKT条件、朗格朗日对偶问题、核函数、SMO（Sequential minimal optimization）算法，如果答的顺畅，可以继续问支持向量机损失函数的具体推导过程，标准答案在下面的第三节进阶问题里有。

**问：说一下逻辑回归模型的原理**

答：关键词 - 样本独立假设、伯努利分布、最大似然函数或极大似然估计、交叉熵，如果答的顺畅，可以继续问逻辑回归模型损失函数的具体推导过程，标准答案在下面的第三节进阶问题里有。

**【附加！】近5年开始很多同学会打机器学习比赛，比如阿里天池、Kaggle竞赛等，其中最常会用到GBDT，XGBoost，可以问以下两个问题**

**问：GBDT的优点和局限性**

答：优点：1. 在分布稠密的数据集（大数据）上，泛化能力和表达能力都很好（经验风险和结构风险都比较小），这使得GBDT成为打比赛神器（当然，近两年已经基本被XGBoost取代）；2. 由于树与树之间可以并行化计算，所以预测阶段计算速度很快；3. GBDT采用决策树作为弱分类器，具有十分良好的的解释性（因为决策树的每个节点我们知道实际物理含义）和鲁棒性（弱分类器集成具备的效果），能够自动发现特征间的高阶关系，并且不需要对数据进行特殊的预处理如归一化（其它绝大多数模型需要，基于决策树的模型基本都不需要，因为决策是通过阈值判定的，所以不同维度的取值范围差别过大没影响）。

局限性在于：1. 在高维稀疏的数据集上，表现不如支持向量机或者神经网络；2. 在处理文本分类特征问题（或者说类别型特征，因为这种特征通常不具备大小关系）上，相对其他模型的优势不如它在处理数值特征时明显；3. 训练阶段需要串行训练（没办法，Boost类机器学习模型都需要串行训练，需要前一个分类器迭代后，再迭代后一个，如下图所示），只能在决策树内部采用一些局部并行的手段提高训练速度。

<img src="https://kangcai.github.io/img/in-post/post-interview/i5.png"/>

**问：XGBoost与GBDT的联系和区别**

答：XGBoost对GBDT进行了算法和工程上的许多改进：算法上主要有以下两点改进，1. XGBoost 在决策树构建阶段就加入了正则项，2. 不同于IC3，C4.5，CART算法，XGBoost设计了自己独有的准则选取最优分裂，而GBDT采用的CART算法；工程实现上做了大量优化，提升效果和效率，主要有以下三点：1. GBDT采用CART决策树作为基分类器，而XGBoost支持多种类型的基分类器，甚至线性分类器；2. GBDT 在每轮迭代时使用全部数据，XGBoost采用了与随机森林（Random Forest）相似的策略，支持对数据进行采样；3. GBDT 没有设计对缺失值的处理，XGBoost 能够自动学习出缺失值的处理策略。

## 三、传统机器学习进阶问题

【传统机器学习进阶问题，基础问题如果都不会，这里的问题就可以不用继续问】

**问：空间上线性可分的两类点，分别向SVM分类的超平面上做投影，这些点在超平面上的投影依然线性可分吗？**

答：如果面试者不懂题干，表示其不懂SVM原理；本题答案是不能，理由是如果在超平面上线性可分的话，脑补一下可以知道该超平面可以旋转一下达到更优，当然严格证明比较麻烦，不放上来了。

**问：推导一下支持向量机的损失函数，不用考虑软间隔问题，不用考虑正则化。**

答：SVM 的优化目标是使样本到超平面的最小间隔最大，首先点到平面的距离公式可表示如下，

<img src="https://kangcai.github.io/img/in-post/post-interview/i6.png"/>
<img src="https://kangcai.github.io/img/in-post/post-interview/i7.png"/>

**问：推导一下逻辑回归的损失函数**

<img src="https://kangcai.github.io/img/in-post/post-interview/i8.png"/>
<img src="https://kangcai.github.io/img/in-post/post-interview/i9.png"/>

**问：LDA（Linear Discriminant Analysis，线性判别分析） 和 PCA（Principal Components Analysis，主成分分析） 作为经典的特征降维算法，有何区别？**

答：LDA是有监督算法，即样本需要有类标签，该算法思想是使降维投影后的样本点满足最大化类间距离和最小化类内距离的条件，具体来说，是使类间距Sb尽可能的大、类内散度Sw尽可能的小，目标是最大化 tr(W^T Sb W) / tr(W^T Sw W)，其中 W是待求解的降维投影超平面，W^T是W的转置，tr 是the trace of the matrix，即矩阵的迹。PCA是无监督算法，即样本不需要有类标签，该算法思想是使降维投影后的样本点方差最大。

**问：PCA的算法思想是使降维投影后的样本点方差最大，那么这个时候样本点到投影的距离有怎么样的特点？**

答：样本点到投影的距离平方和最小，推导过程繁杂，这里省略。

**问：了解梯度下降法吗？梯度下降法的特点（面试者没听懂问题，就问他有什么缺点）？有什么方法可以解决它在最优解附近以较大幅度震荡无法收敛的问题？（可以进一步问动量梯度下降法的优缺点）**

答：优点：求一阶导，操作简单，计算量小，在损失函数是凸函数的情况下能够保证收敛到一个较好的全局最优解。缺点：1. α是个定值，收敛的好坏受它的直接影响，过小会导致收敛太慢，过大会导致震荡而无法收敛到最优解。2. 对于非凸问题，只能收敛到局部最优，并且没有任何摆脱局部最优的能力，因为一旦梯度为0就不会再有任何变化。
	解决最优解附近以较大幅度震荡无法收敛的问题通常有以下4种方法：
1. 方法1，手动调整学习率，这个应该很多同学会提到，
2. 方法2，动量（Momentum）梯度下降法，优点： 1. 由于在迭代时考虑了上一次 θ 的迭代变化量，所以在最优解附近通常具有梯度因方向相反而抵消的效果，从而在一定程度上缓解 SGD 的收敛不稳定问题；2. 除此之外，Momentum 还具有一定的摆脱局部最优的能力，如果上一次梯度较大，而当前梯度为0时，仍可能按照上次迭代的方向冲出局部最优点。缺点：又多了另外一个超参数 ρ 需要我们设置，它的选取同样会影响到结果。

 <img src="https://kangcai.github.io/img/in-post/post-interview/i10.png"/>

3. 方法3，Nesterov Momentum 又叫做 Nesterov Accelerated Gradient（NAG） ，是基于 Momentum 的加速算法。NAG认为：既然都把上一次迭代向量作为本次预期向量了，就应该预期到底，应该将当前梯度替换成预期梯度，即当前 θ 加上上一次迭代向量之后的新 θ 所在点的函数梯度。NAG可以看成是Momentum的激进版。
4. 方法4，共轭梯度法（Conjugate Gradient），梯度下降法一次迭代是针对一个维度方向进行下降，故它每一步都是垂直于上一步。而共轭梯度法本质是把目标函数分成许多方向，然后不同方向分别求出极值再综合起来 。优点：在n维的优化问题中，共轭梯度法最多n次迭代就能找到最优解。共轭梯度法其实是介于梯度下降法与牛顿法之间的一个方法，是一个一阶方法，但克服了梯度下降法收敛慢的缺点，又避免了存储和计算牛顿法所需要的二阶导数信息。缺点：需要求解共轭，高维向量计算量过大，故不适用于高维参数向量的情况。

**问：了解牛顿迭代法吗？牛顿迭代法相比梯度下降法的优点？牛顿迭代法存在的问题？有什么方法可以解决这个问题？**

答：牛顿迭代法优点：二阶收敛，收敛速度快。二阶收敛的牛顿法相比于一阶收敛的梯度下降法，收敛速度更快，因为二阶收敛还额外考虑了下降速度变化的趋势。从几何上解释，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径，路径找对了，下降速度就更快。牛顿迭代法存在的问题：难以计算，牛顿法的迭代过程中，每一步都需要求解目标函数的 Hessian 矩阵的逆矩阵，计算比较复杂甚至是无法计算，这个问题很严重，所以在机器学习中甚至都不会直接使用牛顿法。拟牛顿法（包括DFP 、BFGS 、 L-BFGS）可以解决该问题，拟牛顿法本质思想是在牛顿法的基础上，使用迭代法来获得 Hessian 矩阵的逆矩阵：这样一来，只需要最开始求一次逆，后面就可以通过迭代的方法来获取每一次需要使用的 Hessian 矩阵的逆矩阵，从而简化了迭代运算的复杂度。

**问：了解 VC 理论吗 or 了解可学习性理论吗？随着VC维从尽可能小到尽可能大，模型表达能力怎么变化，模型训练误差怎么变化，模型的泛化误差怎么变化，训练误差和泛化误差的差值怎么变化，过拟合程度怎么变化？**

答：VC维越大，模型表达能力越强，模型的训练误差越小，模型的泛化误差越大，训练误差和泛化误差的差值先变小后变大，过拟合程度越大。

**问：（本问题应该是最难的）如果了解 VC 理论的话，可以问该问题，针对2维数据，做一个2分类任务，泛化误差和训练误差的最大差距允许是 0.1，对应置信度是 90%，所用的模型是线性分类模型，大概需要多少数据用来训练？ or  不需要计算准确数目，说思路即可**

答：首先，对于2维数据的线性二分类任务，d_vc=3，由于有 ε=0.1，δ=1-90%=0.1，根据下面的式子，模型的泛化误差和训练误差有如下关系，

<img src="https://kangcai.github.io/img/in-post/post-interview/i11.png"/>

将前述数据代入到上式，可以得到结果，如下图所示，

<img src="https://kangcai.github.io/img/in-post/post-interview/i12.png"/>

于是我们知道了，大概需要29300条训练数据作为训练集。

## 四、深度学习基础问题

【基本都要掌握的知识，如果大多都不了解，可以判定是只会用】

**问：对于全连接深度神经网络参数初始化一般是怎么设置的？可以全部初始化为0吗？**

答：答取值范围为某个范围的均匀分布 或者 高斯分布 或类似的 都没问题；不行，不能全部初始化为0，更广泛地来说不能初始化完全一样的值，如果初始参数一样，那么无论是前向传播还是反向传播的取值是完全相同的，学习过程永远无法打破这种对称性，网络最终参数仍是相同的，无法进行有效训练。

**问：常见的激活函数有哪些？各自的特点 or 能画一下吗 or 写一下函数表达式吗？**

答：Sigmoid，Tanh，ReLu。Sigmoid 与 Tanh类似，都是饱和的激活函数，即输入值较大或较小时梯度趋近于0，Sigmoid取值从0到1，Tanh取值从-1到1；ReLu是非饱和的激活函数。（注意：下面的图形和公式没有按行对应，左图第一行是tanh，公式第一行是sigmoid的）
 
 <img src="https://kangcai.github.io/img/in-post/post-interview/i13.png"/>

**问：ReLU（线性整流函数，Rectified Linear Unit,目前深度学习领域最常用的激活函数）的优缺点？**

答：优点：1. 计算量小，ReLU只需要判断一个阈值就可得到激活值，而Sigmoid和Tanh都包含了指数计算；2. 非饱和性可以有效解决梯度消失的问题，提供相对宽的激活边界；3. ReLU的单侧抑制提供了网络的稀疏表达能力（就是说使得激活值有许多是0）。缺点：是训练过程中可能使神经元死亡，即激活值永远为0，在实际训练中，如果学习率（Learning Rate）设置较大，会导致一定比例的神经元你不可逆死亡，进而参数梯度无法更新，整个训练过程失败。（为了克服这个缺点，因此人们设计了ReLU的变种 斜坡ReLu，即LReLU；还有参数化ReLU，即PReLU）

**问：深度神经网络抑制过拟合的方式有哪些？**

答：引入正则化（范数惩罚，比如L1正则或L2正则）；dropout；对所有数据进行归一化处理（Normalization）；批量归一化（Batch Normalization），即对网络的每一层输入之前都使用归一化（均值为0，标准差为1的处理）；增加样本量；网络参数共享（比如卷积神经网，Convolutional Neural Networks，CNN就是这样的）；提前终止训练。

**问：dropout 为何能起到抑制过拟合的作用？它具体的工作原理和内部实现。**

答：（理解dropout：dropout是指深度网络的训练中，以一定的概率随机地临时丢掉一部分神经元（神经元的值置为0），具体来讲dropout作用于每份小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络，可以看作是一种轻量级的Bagging集成近似）

能抑制过拟合的原因：通过dropout，对于任意神经元，每个批次的训练都会与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

工作原理和内部实现包括训练和预测两个阶段，训练阶段按预先设定的dropout概率p生成一个维度是神经元个数、取值为0或1的向量，代表对应的各个神经元是否需要被丢弃，如果被丢弃，则该批次不会计算梯度或参与后面的误差传播；预测阶段是前向传播的过程，在这个过程中，每个神经元的参数要预先乘以概率系统p，以恢复在训练中该神经元只有p的概率被用于bp的事实（预测阶段的这一部分面试者很有可能不了解）

**问：了解卷积神经网络（Convolutional Neural Networks，CNN）吗？卷积神经网络常用的池化（Pooling）操作有哪些？池化的作用是什么？**

答：让面试者答卷积神经网络的流程，一般都能说出 卷积-池化-卷积-池化- … -全连接层-Softmax或直接输出。

常用的池化（Pooling）操作：均值池化（mean pooling）、最大池化（max pooling），这两个是常见，然后特殊的还有重叠池化，空间金字塔池化；
池化操作的作用是显著降低参数，还能保持对图像平移、伸缩、旋转操作的不变性，这3个不变性对图像处理来说是十分基本和重要的。

**问：了解循环神经网络（Recurrent Neural Network, RNN）吗？他相比于一般的前馈神经网络有何特点？循环神经网络存在什么问题？有什么办法解决这个问题？**

答：循环神经网络每一层隐层除了接受输入到当前层的变量 x （一般神经网络都这样）外，还额外接受上一次隐层状态，一起计算输出，作用是拥有了一定的记忆能力，即在做当前的训练的时候，还编码了之前的状态；循环神经网络存在梯度消失或梯度爆炸的问题，这也是一般神经网络的问题共性；解决循环神经网络的梯度消失或梯度爆炸的问题，通常是使用长短时记忆网络（Long Short  Term Memory，LSTM），（也有可能同学会提到Gated Recurrent Unit，GRU，LSTM的简化优化）该模型在RNN的基础上加入了三个门和一个记忆单元：输入门控制当前计算的新状态以多大程度更新到记忆单元，遗忘门控制前一步记忆单元中信息有多大被遗忘掉，输出门控制当前的输出有多大程度上取决于当前的记忆单元，这个模型是目前广泛应用在语音识别、语言建模、机器翻译这些方向上。

**问：了解生成式对抗网络（Generative Adversarial Networks，GANs）吗？说一下基本思想和训练过程？**

答：主要框架包含 生成器（Generator）和 判别器（Discriminator）两个部分。其中生成器用于合成“假”样本，判别器用于判断输入的样本是真还是假的。生成器尽可能造出样本迷惑判别器，而判别器则尽可能识别出来自生成器的样本，以对抗的方式，让两方不断成长，最终能达到一种平衡，双方接近完美而没有更进一步的空间。训练过程主要采用 生成器和判别器交替优化的方式，首先，固定生成器，利用其产生的随机样本作为负样本，并从真实数据集中获取一批正样本，一起输入到判别器中，通过BP（Error Back-propagatio，误差反向传播）算法，更新判别器的参数，然后，固定判别器，利用生成器随机模拟的样本作为输入，输入到判别器中，同样通过BP算法更新生成器。

## 五、深度学习进阶问题

【如果基础问题不了解，这些问题不用问】

**问：平方误差损失函数和交叉熵损失函数在神经网络中分别适合什么场景？及其原因呢？**

答：平方损失函数更适合输出为连续，并且最后一层不含 sigmoid 或 softmax 激活函数的神经网络；交叉熵损失函数则适合二分类或多分类的场景，也就是适合 sigmoid 或 softmax 激活函数的神经网络。原因在于， sigmoid 或 softmax 函数当输入值偏大或偏小时，导数（梯度）会很小，会导致学习速率很慢；而这种情况使用交叉熵损失函数，导数（梯度）是线性，不会导致学习速率过慢。

**问：单层感知机是线性还是非线性模型？输入是二维的情况下，单层感知机可以表示异或逻辑吗？输入是二维的情况下，多层感知机要表示异或逻辑至少需要几个隐层？需要几个节点？**

答：单层感知机是线性模型；不能，线性模型都不能表示异或逻辑；一层就行；输入是二维的情况下这一层隐层也只需要两个节点，另外，多层感知机就是非线性模型。

**问：LSTM相比与同网络结构的RNN，参数数目有什么变化？**

答：LSTM额外多出来三个门，每个门多了一个 U（与当前输入 x 相乘的），W（与上一次隐层状态相乘的），还有一个偏置 b，即多了 3U，3W，3b；

**问：LSTM模型各模块分别使用什么激活函数？那可以用别的激活函数吗？**

答：三个门用的是 Sigmoid，因为它的输出范围是0 - 1之间，符合门的物理定义；记忆单元用的是 Tanh，因为它的输出范围是-1 - 1 之间，这与特征分布以0为中心吻合，而且因为 Tanh 函数在输入0附近相比于 Sigmoid 梯度更大，收敛更快；一般答案就是不用别的激活函数，使用别的激活函数没这种组合好，因为 Sigmoid 和 Tanh 都是饱含的激活函数，即输入达到一定大或小的值时，输出不会明显变化，而 Relu 就难以实现门控的效果，但也有其它特殊情况，比如嵌入式设备对计算量有要求，可能就会简化 Sigmoid 为 01 离散函数。

**问：生成式对抗网络（Generative Adversarial Networks，GANs）与传统概率生成模型方法（比如马尔可夫随机场，贝叶斯网络）的关系或优势？**

答：传统概率生成模型方法要定义一个概率分布表达式P(X)，通常是一个多变量联合概率分布的密度函数P(X1,X2,…Xn)，并基于此做最大似然估计，当随机变量很多时，概率模型会变得十分复杂，概率推断计算量十分巨大，很难计算出好的效果。而GANs不对P(X)直接建模，而是通过制造样本x，间接体现分布P(X)，因此避开了大量复杂的概率计算。