---
layout: post
title: "机器学习·总览篇 IV"
subtitle: "机器学习的三要素"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 机器学习
  - 机器学习·总览篇
---

> 从基本理论上看，如前一篇介绍统计推断的文章所说，统计推断是机器学习十分重要的理论基础，所以机器学习狭义上就是指代统计学习方法；从方法框架上看，机器学习（统计学习方法）的组成有三个要素：模型、策略和算法。我们通常接触到的所谓“深度学习”、“支持向量机”、“罗吉斯蒂克回归”、“决策树”等等，都离不开这三要素。毫不夸张的说，学习机器学习的关键，就是掌握它这三个要素。

> 本文首发于我的知乎专栏[《机器怎么学习》](https://zhuanlan.zhihu.com/machine-learning-complete)中 [机器学习·总览篇(4) 机器学习的三要素](https://zhuanlan.zhihu.com/p/48521073)，转载请保留链接 ;)

### 一、机器学习的三要素

数据在机器学习方法框架中的流动，会按顺序经历三个过程，分别对应机器学习的三大要素：1. 模型；2. 策略；3. 算法

* 模型：谈到机器学习，经常会谈到机器学习的“模型”。在机器学习中，**模型的实质是一个假设空间（hypothesis space），这个假设空间是“输入空间到输出空间所有映射”的一个集合，这个空间的假设属于我们的先验知识。然后，机器学习通过“数据+三要素”，目标是获得假设空间的一个最优解，翻译一下就是求模型的最优参数**。

* 策略：在模型中提到，机器学习的目标是获得假设空间（模型）的一个最优解，那么问题来了，如何评判优还是不优？**机器学习三要素之中的策略就是评判“最优模型”（最优参数的模型）的准则或方法**。

* 算法：

### 二、模型

##### 2.1 判别模型和生成模型

一般来说，机器学习模型会分为判别模型（Discriminative Model）和生成模型（Generative Model）两类。

判别模型更常用，感知机（Perceptron）、罗吉斯蒂克回归（LR）、支持向量机（SVM）、神经网络（NN）、K近邻（KNN）、线性判别分析（LDA）、Boosting、条件随机场（CRF）模型都属于判别模型。判别模型本身又分为两类：（1）直接对输入空间到输出空间的映射建模；（2）对条件概率P(y|x)建模，再分类。

生成模型是一种更加间接的建模，高斯判别分析（GDA）、朴素贝叶斯（NB）、文档主题生成模型（另外一个LDA，跟线性判别分析的LDA是完全不同的两个概念）、受限玻尔兹曼机（RBM）、隐马尔科夫模型（HMM）属于生成模型。分三步：先对联合概率P(x,y)建模，再根据贝叶斯公式算出P(y|x)，最后再分类。

以上提到了很多种当前热门的机器学习模型，有不了解的也不要紧，在后面的文章都会一一介绍。而所谓的深度学习，它的深度神经网络模型（DNN）也只是众多机器学习模型中的一类：卷积神经网络（CNN）、循环神经网络（RNN）、多层感知机（MLP）、堆叠自编码器（SAE）、堆叠受限玻尔兹曼机（有一个专门名字深度置信网络，DBN），以及以上各种模型的变形和扩展，它们都只是针对特定的问题和当前的高性能计算时代而诞生，没必要神化。为了方便起见，后文提到的某些模型的名称用惯用简写来表示。

回到判别模型和生成模型，学术界求条件概率P(y|x)的方法应该直接建模还是间接建模有分歧，SVM之父Vapnik的观点是生成模型的第一步是先对联合概率P(x,y)建模，这个做法没必要，对P(y|x)直接进行建模就行了；Andrew Ng却为生成模型发声，他认为对P(x,y)进行建模从而达到判别的目的也有它自身的一些优势。

##### 2.2 概率模型和非概率模型

还有一种常用的分类方式，将判别模型（2）方法即先求条件概率的方法和生成模型先求联合概率的方法作为一类，称之为概率模型；将判别模型（1）方法即直接建模的方法分为另一类，称之为非概率模型。

概率模型由条件概率分布P(y|x)表示。所有生成模型都是概率模型，除此之外，判别模型中的LR、条件随机场（CRF）等属于概率模型。概率模型指出了学习的目的是学出联合概率P(x,y)或条件概率P(y|x)，其中联合概率通过贝叶斯公式P(x,y)=P(x|y)P(y)拆分成P(x|y)和P(y)分别进行估计。无论是P(y|x)、P(x|y)还是P(y)，都是会先假设分布的形式，例如LR就假设了 y|x 服从伯努利分布，线性回归假设了误差项服从均值为0的高斯分布。

非概率模型由决策函数y=h(x)表示。判别模型中感知机、SVM、神经网络、KNN都属于非概率模型。

### 三、策略

了解机器学习的策略，最关键是掌握10个名词：欠拟合（Underfitting）、过拟合（Overfitting）、经验风险（Empirical risk）、经验风险最小化（Empirical risk minimization, ERM）、结构风险（Structural risk）、结构风险最小化（Structural risk minimization, SRM）、损失函数（Loss function）、代价函数（Cost function）、目标函数（Object function）、正则化（Regularization）

为了理解这些名词，我们从一个例子开始说起，如图X所示，

<img src="https://kangcai.github.io/img/in-post/post-ml/fitting.png"/>
<center>图X 数据拟合</center>

其中，蓝色的线为我们训练出的模型，红叉是样本。从图X中可以看到，第1个子图中模型没有很好的拟合样本，这种情况就叫做**欠拟合（Underfitting）**，欠拟合的问题在于无法很好地拟合当前训练样本。

为了避免欠拟合的问题，需要有一个标准来表示我们拟合的好坏，通常用一个函数来度量拟合的程度，比如十分常用的平方损失函数（Square loss function），图X是对某一个样本得拟合程度函数，

<img src="http://latex.codecogs.com/gif.latex?|y_i-f(x_i)|" title="|y_i-f(x_i)|" />
<center>图X Square Loss function</center>

这个函数就称为**损失函数(loss function)**。损失函数越小，就代表模型对某个样本拟合得越好。风险函数是损失函数的期望，这是由于我们输入输出遵循一个联合分布，但是这个联合分布是未知的，所以无法计算。但是我们是有历史数据的，就是我们的训练集，关于训练集的平均损失称作**经验风险(Empirical risk)**，即，

<img src="http://latex.codecogs.com/gif.latex?\frac{\sum_{i=1}^{N}|y_i-f(x_i)|}{N}" title="\frac{\sum_{i=1}^{N}|y_i-f(x_i)|}{N}" />

所以我们的目标就是最小化，称为**经验风险最小化（Empirical risk minimization, ERM）**，而这个函数称为**代价函数（Cost function）**。

如果到这一步就完了的话，那我们看上面的图，那肯定是图X中的经验风险函数最小了，因为它对历史的数据拟合的最好嘛。但是我们从图上来看。肯定不是最好的，因为它过度学习历史数据，导致它在真正预测时效果会很不好（比如此时右上方按照目前的趋势再有一堆新样本，该模型就废掉了），这种情况称为**过拟合（Overfitting）**。为什么会造成这种结果？大白话说就是它的函数太复杂了，都有四次方了，这就引出了下面的概念，我们不仅要让经验风险最小化，还要让结构风险最小化。这个时候就定义了一个函数

<img src="http://latex.codecogs.com/gif.latex?J(f)" title="J(f)" />

，这个函数专门用来度量模型的复杂度，表示模型的**结构风险（Structural risk）**在机器学习中也叫**正则化(Regularization)**，为了让模型尽可能结构简单，我们的优化目的又多一个，即**结构风险最小化（Structural risk minimization）**。常用的正则化方法有L1范数和L2范数，在机器学习中正则化使用得极其广泛，以后的文章会进行详细介绍。到这一步我们就可以说我们最终的优化函数是：

<img src="https://kangcai.github.io/img/in-post/post-ml/object function.png"/>

即经验结构风险的表示，而这个函数就被称为**目标函数（Object function）**。这个时候回过头来看图X：最左面的结构风险最小（模型结构最简单），但是经验风险最大（对历史数据拟合的最差）；右边的经验风险最小（对历史数据完全拟合），但是结构风险最大（模型结构最复杂）；中间的，达到了二者的良好平衡，即经验结构最小化，最适合用来预测未知数据集。

虽然上文对损失函数（Loss function）、代价函数（Cost function）、目标函数有介绍，但在很多情况下，甚至包括paper里，损失函数、代价函数、目标函数的叫法并不严格，可能存在混用的情况，但三个名词如果同时出现，还是应该按上述定义理解，这里再总结一下：

* 损失函数 <img src="http://latex.codecogs.com/gif.latex?|y_i-f(x_i)|" title="|y_i-f(x_i)|" /> ，一般是针对单个样本 _i_
* 代价函数 <img src="http://latex.codecogs.com/gif.latex?\frac{\sum_{i=1}^{N}|y_i-f(x_i)|}{N}" title="\frac{\sum_{i=1}^{N}|y_i-f(x_i)|}{N}" />，一般是针对总体，即在损失函数基础上的总体样本均值
* 目标函数 <img src="http://latex.codecogs.com/gif.latex?\frac{\sum_{i=1}^{N}|y_i-f(x_i)|}{N}&space;&plus;&space;[L_1|L_2]" title="\frac{\sum_{i=1}^{N}|y_i-f(x_i)|}{N} + [L_1|L_2]" />，即在代价函数基础上加正则化项

### 四、算法



参考文献

1. [《统计学习方法》 李航][1]
2. [On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes. AY Ng 2002][2]

[1]: (https://book.douban.com/subject/10590856/)
[2]: (https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)
