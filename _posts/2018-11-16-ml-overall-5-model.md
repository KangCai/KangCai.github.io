---
layout: post
title: "机器学习·总览篇 V"
subtitle: "三要素之模型"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 机器学习
  - 机器学习·总览篇
---

> 上一篇文章介绍了机器学习的三要素-模型、策略、算法，但只是比较粗略地介绍，本文将着重介绍三要素之模型，视角主要放在不同模型间的区别

> 本文首发于我的知乎专栏[《机器怎么学习》](https://zhuanlan.zhihu.com/machine-learning-complete)中 [机器学习·总览篇(5) 三要素之模型](https://zhuanlan.zhihu.com/p/48914251)，转载请保留链接 ;)


机器学习首要考虑的问题是学习什么样的模型，如在《机器学习·总览篇(4) 机器学习的三要素》中描述的，模型就是所要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布和决策函数，一般是无限多种可能性，训练（数据经过模型、策略、算法的处理，对模型的参数进行调整）的目的就是获取一个最优参数的模型。

### 非概率模型、概率判别模型、生成模型

模型通常两种常见的分类方式，

* **按模型形式分类：概率模型（Probabilistic Model）和 非概率模型（Non-probabilistic Model）**

* **按是否对观测变量的分布建模分类：判别模型（Discriminative Model）和 生成模型（Generative Model）**

上述两种分类方法相当于把所有模型划分成了三类，可用图1表示，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/Model classification.png"/>
</center>
<center>图1 模型分类示意图</center>

从图1可以看到，从左到右分别是非概率模型（Non-probabilistic Model）、概率判别模型（Probalilistic Discriminative Model）、生成模型（Generative Model）。其中非概率模型和概率判别模型同属于判别模型（Discriminative Model），概率判别模型和生成模型同属于生成模型（Generative Model）。具体分析三种模型，

I. **非概率模型（Non-probabilistic Model）**：直接对输入空间到输出空间的映射y=h(x)建模。实例：感知机（单层神经网络，Perceptron）、多层感知机（MLP）、支持向量机（SVM）、K近邻（KNN）

II. **概率判别模型（Probalilistic Discriminative Model）**：直接对后验概率P(y\|x)建模，然后实现分类。实例：逻辑回归（LR）、最大熵模型（ME）、条件随机场（CRF）

III. **生成模型（Generative Model）**：对联合概率P(x,y)建模。如果是在聚类任务中，到这一步就结束了；但如果在分类任务中，再根据贝叶斯公式算出条件概率P(y\|x)，最后实现分类。实例：高斯判别分析（GDA）、朴素贝叶斯（NB）、受限玻尔兹曼机（RBM）、隐马尔科夫模型（HMM）

**非概率模型(I)和概率判别模型(II)**

两者同属于判别模型，两者共同点如下：

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/data_visual-dm.png"/>
</center>
<center>图2 判别模型分类示意图</center>

如图2所示是判别模型的分类示意图，**非概率模型和概率判别模型的共同点是对如图中划分数据的线一样的判别标准直接建模**。

非概率模型和概率判别模型的差别如下：

1. **非概率模型的判别标准是一个超平面，超平面分割数据起到判别作用；**
2. **概率判别模型的判别标准是一个由条件概率表示的判别函数，根据条件概率的值大小比较起到判别作用。**

**生成模型(III)**

生成模型与上述两种模型（判别模型）的差别如下：

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/data_visual-gm.png"/>
</center>
<center>图3 生成模型分类示意图</center>

如图3所示，分类任务中，**生成模型在类内进行联合概率分布的建模，以此为基础再进行分类；而非概率模型和概率判别模型都不对类内进行联合概率分布的建模**。

**小结**

一个stackoverflow的简单例子能很好地说明以上三种模型的特点。假设特征x是1维，并且值只有1和2可选；标签y的也是1维，且值只有0和1可选，现在有如下 (x,y) 形式的4个样本 - (1,0)、(1,0)、(1,1)、(2, 1)，则学习到的联合概率分布（对应第3类模型，即生成模型）如下：

|  | y = 0 | y = 1| 
| :-----------:| :----------: | :----------: |
| x = 1 |1/2|1/4|
| x = 2 |0|1/4|

而学习到的条件概率分布（对应第2类模型，即概率判别模型，既是概率模型，又是判别模型）如下：

|  | y = 0 | y = 1| 
| :-----------:| :----------: | :----------: |
| x = 1 |2/3|1/3|
| x = 2 |0|1|

而学习到的直接映射（对应第1类模型，即非概率模型）如下：

|  | y = 0 | y = 1| 
| :-----------:| :----------: | :----------: |
| x = 1 |1|0|
| x = 2 |0|1|

可以很清楚地看到信息量从多到少：生成模型 \> 概率判别模型 \> 非概率模型。更全面地总结可用下表表示，

|  | 直接目标 | 数据量 | 模型信息 | 准确率 | 示例 |
| :-----------:| :----------: | :----------: | :----------: | :----------: | :----------: |
| 非概率模型 | 分类 | 少 | 少 | 相对较高| SVM/NN/KNN |
| 概率判别模型 | 后验概率 | 中 | 中 | 相对较高|LR/ME/CRF |
| 生成模型 |联合概率 | 多 | 多 |相对较低| GDA/NB/HMM |

### 线性模型和非线性模型

x是模型函数的自变量，y是模型函数的因变量，上文中对于生成模型和判别模型本质差别是对于y是间接求解还是直接求解，对于概率模型和非概率模型的本质差别是y的表示形式，而在本小节中，我们还可以根据y与x的关系来区分线性模型和非线性模型。

**线性模型**

典型的线性模型有线性SVM和感知机（单层NN），都可以用y=wx+b的形式表示，y与x的关系是线性的；进行分类任务时，数据的分割界面都是线性平面。

对于逻辑回归模型，y=sigmoid(wx+b)，y与x的关系是非线性的；然而，进行分类任务时，数据的分割界面仍然是线性平面。逻辑回归模型实际上我们称之为广义线性模型（Generalized Linear Model，GLM），GLM是线性模型的扩展，逻辑回归模型是y服从伯努利分布下的GLM；线性模型也可以看成是一种特殊的GLM，可以看成是y服从高斯分布下的GLM。GLM通过采用对应的联系函数针对不同y分布的数据，让y的取值范围与预测值范围一致，以及让模型比较好地拟合当下的数据。

其实，在统计意义上，如果一个回归方程是线性的，那么它的相对于参数就必须也是线性的；如果方程相对于参数是线性，那么即使性对于样本变量的特征是二次方或者多次方，这个回归模型也是线性的。所以广义线性模型本质上还是线性模型。后面会在有监督学习篇中详细介绍广义线性模型。

**非线性模型**

以上常见的线性模型通过特定的非线性变换，可以成为处理非线性可分数据的非线性模型：

|  | 通常采用的非线性变换方法 | 
| :-----------:| :----------: | 
| SVM | 非线性核函数|
| LR | 非线性核函数 |
| NN | 多层堆叠 |

还有一些常见的天然非线性模型，如KNN、决策树（DT）、随机森林（RF）、GBDT等。另外，朴素贝叶斯模型用于分类时，假设对于每一维x和每一类y，x\|y 条件概率满足高斯分布：任意维x在不同类别y下，如果 x\|y 分布的方差都相等，则是线性分类器；如果 x\|y 的方差不同，则是非线性分类器，具体分析可参考[朴素贝叶斯分类器本质上是线性分类器][8]一文。


1. [《统计学习方法》 李航][1]
2. [Stanford CS 229 ― Machine Learning][2]
3. [zhihu: 机器学习系列-广义线性模型][3]
4. [机器学习中线性模型和非线性的区别][4]
5. [csdn: 机器学习之核函数逻辑回归（机器学习技法）][7]
6. [souhu: 说人话的统计学][6]
7. [jianshu: 机器学习面试之最大熵模型] [7]
8. [jianshu: 朴素贝叶斯分类器本质上是线性分类器][8]

[1]: (https://book.douban.com/subject/10590856/)
[2]: (https://stanford.edu/~shervine/teaching/cs-229.html)
[3]: (https://zhuanlan.zhihu.com/p/24967776)
[4]: (https://blog.csdn.net/wbcnb/article/details/78306970)
[5]: (https://blog.csdn.net/qq_34993631/article/details/79345889)
[6]: (https://www.sohu.com/a/228212348_349736)
[7]: (https://www.jianshu.com/p/e7c13002440d)
[8]: (https://www.jianshu.com/p/469accb2e1a0)