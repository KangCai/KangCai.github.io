---
layout: post
title: "机器学习·总览篇 VI"
subtitle: "三要素之策略-损失函数"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 机器学习
  - 机器学习·总览篇
---

> 上一篇文章和本文主要介绍三要素的第二要素-策略：上一篇文章中已经介绍了策略中平衡经验风险和结构风险的正则化系数，衡量经验风险的代价函数和损失函数；本文继续介绍最后一个重要的内容，衡量结构风险的正则化项。

> 文章首发于[我的博客](https://kangcai.github.io/)，转载请保留链接 ;)


在上一篇文章中提到过，**策略部分就是评判“最优模型”（最优参数的模型）的准则和方法**，图1是目标函数的函数形式，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/object function 1.png"/>
</center>

如图1所示，目标函数包含了表征经验风险的代价函数和表征结构风险的正则化项，上一偏已经介绍**损失函数**，代价函数是样本集的损失函数之和；正则化是对wx+b中参数w的约束，下文将主要介绍**正则化**。


### 正则化

正则化，即Regularization。wiki中给出了对正则化的很好的解释：

1. 正则化对复杂性施加惩罚
2. 理论依据是试图将奥卡姆剃刀原则加入到求解过程
3. 从贝叶斯的观点来看，不同的正则化实现对应着对模型参数施加不同的先验分布
4. 可以将规则化作为一种技术来提高学习模型的泛化能力，即提高模型对新数据的预测能力

**正则化是对y=wx+b中参数w的约束**，通常按照惯例是不管b的，但如果在实际应用时在正则化函数里将b也加上的话，影响不大，效果基本没差别。正则化对参数w的约束具体对于模型有什么好处？如上一篇文章所说的，**正则化是为了减小结构风险，即使模型复杂度降低，降低过拟合**。参数w和模型复杂度关系可以参考以下两个问题，

**1）参数越稀疏代表模型越简单吗？是否还有其它好处？**

是的。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就可能较差。另一个好处是参数变少可以使整个模型获得更好的可解释性，而且可以用做特征选择。

**2）参数值越小代表模型越简单吗？**

是的。因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数，因此复杂的模型，其参数值会比较大。相反地，参数小，导数小，对于较大波动的样本特征具有鲁棒性。

根据这两个准则，我们可以分析3种常见范数对参数的约束，进而对模型复杂度和其它方面的影响。

##### 1.1 L0范数

L0是向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。

但不幸的是，L0范数的最优化问题是一个NP hard问题，而且理论上有证明，L1范数是L0范数的最优凸近似，因此通常使用L1范数来代替。

##### 1.2 L1范数


L1范数指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。

L1正则化之所以可以防止过拟合，是因为L1范数就是各个参数的绝对值相加得到的，我们前面讨论了，参数值大小和模型复杂度是成正比的。因此复杂的模型，其L1范数就大，最终导致损失函数就大，说明这个模型就不够好。


##### 1.3 L2范数


L2范数是向量中各个元素的平方和，也叫“岭回归”（Ridge Regression）。

与L1范数不一样的是，它不会是每个元素为0，而只是接近于0。越小的参数说明模型越简单，越简单的模型越不容易产生过拟合现象。

##### 1.4 L1范数和L2范数的比较

当特征参数为1维时，L1和L2的函数曲线如图X所示，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/l1 and l2.png"/>
</center>

考虑策略目标函数的原始形式，意味着在g(θ)<=0的约束下，使f(x,θ)最小，其中对于L1范数有g(θ)=TODO，对于L2范数有g(θ)=TODO。当f(x,θ)在无约束的条件下取最优解θ本身就满足g(θ)<=0，约束等于没有，这种情况不考虑；**现在考虑另外一种情况，f(θ)在无约束条件下最优解θ不满足g(θ)<=0，此时L1和L2应用于目标函数最优解求解的示意图可用图X表示，为了方便可视化，这里考虑特征参数为2维的情况**，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/L1&L2.png"/>
</center>

如图X所示，f(θ)在无约束条件下最优解θ不满足g(θ)<=0，即绿色圆圆心在蓝色区域外，f(θ)在g(θ)<=0约束的最优解必然在两者的边界上，且g(θ)；由于此时是最小值，所以导数满足f'(x,θ)+λ·g'(θ)=0；又由于f'(x,0)与g'(θ)在最小值处符号相反，故λ>=0。

##### 1.5 

[wiki: Regularization (mathematics)](https://en.wikipedia.org/wiki/Regularization_(mathematics))
[cnblogs: 机器学习之正则化](https://www.cnblogs.com/jianxinzhou/p/4083921.html])
[cnblogs: 机器学习中的范数规则化之（一）L0、L1与L2范数](https://www.cnblogs.com/weizc/p/5778678.html)

