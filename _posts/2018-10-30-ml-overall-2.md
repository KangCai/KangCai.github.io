---
layout: post
title: "机器学习·总览篇(2)"
subtitle: "机器学习的发展历程"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 机器学习
  - 机器学习·总览篇
---

> 学习任何领域，"了解"总是第一步，而认识该领域的发展历程是"了解"一个领域十分有效的方法


### 机器学习的发展历程

机器学习的发展和人工智能发展是离不开的，机器学习是人工智能研究发展到一定阶段的必然产物。

人工智能的研究历史有着一条从以“推理”为重点，到以“知识”为重点，再到以“学习”为重点的自然、清晰的脉络。下面是人工智能发展的三个时期:

* 推理期: 时间为1950s~1970s，人们认为只要给机器赋予逻辑推理能力，机器就能具有智能。这一阶段的代表性工作主要有A. Newell和H. Simon的“逻辑理论家”程序以及此后的“通用问题求解”程序等，这些工作在当时取得了令人振奋的成果。例如，“逻辑理论家”程序在1952年证明了著名数学家罗素和怀特海的名著《数学原理》中的38条定理，在1963年证明了全部的52 条定理，而且定理 2.85甚至比罗素和怀特海证明得更巧妙。A.Newell和H.Simon因此获得了1975年图灵奖。然而，随着研究向前发展，人们逐渐认识到，仅具有逻辑推理能力是远远实现不了人工智能的。E.A. Feigenbaum等人认为，要使机器具有智能，就必须设法使机器拥有知识。
* 知识期: 时间为1970s~1980s，在这一时期，大量专家系统问世，在很多领域做出了巨大贡献。E.A. Feigenbaum 作为“知识工程”之父在 1994 年获得了图灵奖。但是，专家系统面临“知识工程瓶颈”，简单地说，就是由人来把知识总结出来再教给计算机是相当困难的。于是，一些学者想到，如果机器自己能够学习知识该多好
* 学习期: 时间为1980s~现在，机器学习开始受到重视，成为一个独立的学科领域并开始快速发展、各种机器学习技术百花齐放的时期。事实上，图灵在1950年提出图灵测试的文章中就已经提到了机器学习的可能，而1950s其实已经开始有机器学习相关的研究工作，主要集中在基于神经网络的连接主义学习方面，代表性工作主要有F.Rosenblatt的感知机、B.Widrow的Adaline等。

如果只关注机器学习发展的这条线，推荐Eren Golge博客中的一张图:

<img src="https://kangcai.github.io/img/in-post/post-ml/history_of_ml.jpg"/>
<center>图1 （一条可供参考的）机器学习时间线</center>

在图1中，按时间轴顺序事件详情如下:

* 1943年【NN基础理论】，McCulloch和Pitts提出了神经网络层次结构模型，确立了神经网络的计算模型理论，从而为机器学习的发展奠定了基础

* 1950年【重要事件】，Turing提出了著名的“图灵测试”，使人工智能成为了科学领域的一个重要研究课题

* 1957年【NN第一次崛起】，Rosenblatt提出了Perceptron（感知器）概念，用Rosenblatt算法对Perceptron进行训练。并且首次用算法精确定义了自组织自学习的神经网络数学模型，设计出了第一个计算机神经网络（NN算法），开启了NN研究活动的第一次兴起

* 1958年【正式LR】，Cox给Logistic Regression方法正式命名，用于解决美国人口普查任务

* 1959年【重要事件】，Samuel设计了一个具有学习能力的跳棋程序，曾经战胜了美国保持8年不败的冠军。这个程序向人们初步展示了机器学习的能力，Samuel将机器学习定义为无需明确编程即可为计算机提供能力的研究领域

* 1960年【NN发展】，Widrow用delta学习法则来对Perceptron进行训练，可以比Rosenblatt算法更有效地训练出良好的线性分类器（最小二乘法问题）

* 1962年【雏形CNN】，Hubel和Wiesel发现了猫脑皮层中独特的神经网络结构可以有效降低学习的复杂性，从而提出著名的Hubel-Wiese生物视觉模型，该模型卷积神经网络（CNN）的雏形，这之后提出的神经网络模型也均受此启迪

* 1963年【雏形SVM】，Vapnik和Chervonenkis发明原始支持向量方法，即起决定性作用的样本为支持向量（SVM算法）

* 1969年【NN第一次停滞】，Minsky和Papert出版了对机器学习研究有深远影响的著作《Perceptron》，其中对于机器学习基本思想的论断：解决问题的算法能力和计算复杂性，影响深远且延续至今。文章中提出了著名的线性感知机无法解决异或问题，打击了NN社区，从那以后NN研究活动直到1980s都萎靡。

* 1980年【重要事件】，在美国卡内基梅隆大学举行了第一届机器学习国际研讨会，标志着机器学习研究在世界范围内兴起，该研讨会也是著名会议ICML的前身

* 1981年【NN第二次崛起】，Werbos提出多层感知机，解决了线性模型无法解决的异或问题，第二次兴起了NN研究

* 1984年【决策树】，Breiman发表分类回归树（CART算法，一种决策树）

* 1986年【决策树】，Quinlan提出ID3算法（一种决策树）

* 1986年【NN的BP算法】，Rumelhart，Hinton和Williams联合在《Nature》杂志发表了著名的反向传播算法（BP算法）

* 1989年【正式CNN】，Yann和LeCun提出了目前最为流行的卷积神经网络（CNN）计算模型，推导出基于BP算法的高效训练方法，并成功地应用于英文手写体识别

* 1995年【正式SVM】，Vapnik和Cortes发表软间隔支持向量机（SVM算法），开启了随后的机器学习领域NN和SVM两大社区的竞争

* 1995年【NN第二次停滞】，自1995年到随后的10年，NN研究发展缓慢，SVM在大多数任务的表现上一直压制着NN，并且Hochreiter的工作证明了NN的一个严重缺陷-梯度爆炸和梯度消失问题

* 1997年【Adaboost】，Freund和Schapire提出了另一种可靠的机器学习方法-Adaboost，

* 2001年【随机森林】，Breiman发表随机森林方法（Random forest），Adaboost在对过拟合问题和奇异数据容忍上存在缺陷，而随机森林在这两个问题上更加鲁棒。

* 2005年【NN第三次崛起】，经过多年的发展，NN众多研究发现被现代NN大牛Hinton, LeCun, Bengio, Andrew Ng和其它老一辈研究者整合，NN随后开始被称为深度学习（Deep Learning），迎来了第三次崛起。

近些年的著名的模型和算法，会在后面每个专门的篇章中再进行详细介绍。

参考文献

1. [wiki: 机器学习][1]
2. [Blog: Brief History of Machine Learning][2]
3. [CSDN博客: 机器学习综述——机器学习理论基础与发展脉络][3]

[1]: (https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)
[2]: (http://www.erogol.com/brief-history-machine-learning/)
[3]: (https://blog.csdn.net/solomon1558/article/details/40798401)