---
layout: post
title: "机器学习 · 总览篇 XI"
subtitle: "特征工程"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 机器学习
  - 机器学习·总览篇
---

> 前面的文章着重介绍了机器学习的三要素，包括模型、策略和算法，但

> 文章首发于[我的博客](https://kangcai.github.io/)，转载请保留链接 ;)

### 一、特征工程是什么

本文为了方便理解，在每种特征工程方法下都举个一个实例进行分析，基本可以表达出各个特征工程方法的思想，但不一定具备很强的代表性，仅起着抛砖引玉、引发思考的作用。

### 二、特征构建

通常有这么一种认知，认为机器学习会彻底占领人类专家擅长的领域，其实并不然，至少现阶段是错误的。特征构建作为机器学习流程的第一步，也是很重要的一步，是十分依赖专家知识的。

特征构建是指从原始数据中人工的找出一些具有物理意义的特征。需要花时间去观察原始数据，思考问题的潜在影响因素：

1. 对领域的认知、数据的敏感性、机器学习实战经验能有效地帮助我们进行特征构建，但这些都需要一定的积累和沉淀。
2. 除此之外，特征构建还有一些及时可以使用的技巧，**数据的分割和结合**：分割是指将某一种数据细化成多种数据，提供更多的信息，提高模型准确率；而结合是相反的过程，将冗余表达的信息合并在一起，提高模型收敛效率。

``【场景1-属性分割】根据大学生前两个学期的表现来预测他是否能在大学毕业时获得优秀毕业生的称号。【分析】直观的当然应该将学习成绩作为一个特征属性，但进一步进行考虑，通常专业必修课课成绩对评选影响更大，而非专业选修课影响会比较小，学习成绩又是本任务一个十分重要的影响因素，所以分开更合理，那么这种情况就是按学科将学习成绩属性分割。``


``【场景2-属性结合】预测大学生毕业后的薪资水平【分析】对于该任务，影响因素大概有学校、专业、学习成绩、获奖情况、课程项目经历、公司实习经历等，在这种情况下影响因素本身就较多，而且都相对比较重要。我们已有的数据包含了学生各科的成绩，如果直接拿来作为特征，根据《总览篇 VII 三要素之策略-正则化》可知，特征过多可能会导致过拟合；而过多太细化的特征实际影响并不大。所以，跟属性分割相反，将所有成绩做平均，或者最多将部分课程成绩结合分成专业课成绩和非专业课成绩是更合适的做法。``

可以看到，特征的构建是一个精准权衡的过程，需要做到信息 **“不冗余的完整”**，是个非常麻烦的问题，书里面也很少提到具体的方法，需要对问题有比较深入的理解。

### 三、特征清洗

##### 4.1 无效值和缺失值的处理

由于各种可能的因素，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法包括：

1. 估算（Estimation）。使用缺失数值所在行或列的均值、中位数、众数、最值等来替代缺失值。或者根据样本其它有相关性的数据来估算缺失数据。
2. 样本删除（Casewise Deletion）。当受到影响的样本占总样本比例较低，或者受影响的特征维度十分重要时，将有问题的样本删除是一种对学习效果影响较小的做法。
3. 变量删除（Variable Deletion）。当受到影响的样本较多，或者受影响的特征维度不太重要时，将有问题的样本删除是一种对学习效果影响较小的做法。有同学会说受影响的样本较多，受影响的特征维度又很重要怎么办？这个时候如果能重新收集数据最好，实在不行就需要在以上三种处理方法中权衡选择了。

``【场景3-无效值和缺失值的处理】根据人的体检指标对疾病自动进行诊断【分析】如果医院数据库的误操作将某一列全部删掉，或者某一位护士负责的那部分体检人的某列数据全部删掉。当缺失值是右眼视力，由于通常人两眼视力是接近的，所以可以用估算的方法，比如直接将左眼视力拷贝到右眼视力；当缺失值是少数体检人的部分重要数据，可以将样本剔除，再进行训练；当缺失值是身高，身高对疾病诊断的重要性不高，所以可以用变量删除的方法，将身高这一维度去除``

采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在数据收集中应当尽量避免出现无效值和缺失值，保证数据的完整性。

##### 4.2 采样

**样本均衡**

在分类中，训练数据不均衡是指不同类别下的样本数目相差巨大。比如对于一个二分类任务，如果训练集中类别1的样本数与类别2的样本数比值为100:1，使用逻辑回归进行分类，那么其结果很可能将所有样本都分类为类别1。样本不均衡会导致训练出的逻辑回归模型无效的原因在于，逻辑回归模型的代价函数是基于最大似然估计（MLE）的，某一类别的样本如果占绝大多数，会导致代价函数基本没有参考意义，因为模型当然会着重保证该类样本分类正确。

实际应用中，训练样本数据不均衡是常见并且合理的，比如在欺诈交易识别中，绝大部分交易是正常的，只有极少部分的交易属于欺诈交易；敏感头像（政治敏感、黄色相关的头像）检测中，绝大部分头像是正常的，只有极少部分头像属于政治敏感、黄色相关的。

通常有如下样本均衡方法：

1. 最直接的方法就是**扩充数据集，去收集更多的少样本类别的样本**；
2. 然后就是**拷贝过采样（over-sampling）**，即对少样本类别进行拷贝过采样来增加类别的样本量，以达到类别间样本均衡。
3. **欠采样（under-sampling）**，即对多样本类别进行欠采样来减少类别的样本量，以达到类别间样本均衡。
4. 人造数据，与拷贝过采样直接拷贝副本的方法不同，这里指的是构造全新样本的方法，代表性方法是SMOTE法，JAIR期刊2002年的文章[《SMOTE: Synthetic Minority Over-sampling Technique》](https://jair.org/index.php/jair/article/view/10302/24590)提出了一种过采样**算法SMOTE，概括来说，本算法基于“插值”来为少数类合成新的样本**。
5. 除此之外，对于boosting算法，它是采用多个弱学习器组合形成的强学习器，所以可以在**进行弱学习器训练时随机选取等量类别的样本**，当然，每个弱学习器都重新选择新的样本。

``【场景】``

**样本权重**

为了解决不同类别样本量差别过大的问题，样本均衡方法是从处理样本本身出发的，样本权重方法是从改变分类算法来实现的，**在代价函数中增加少样本类别的样本权值，降低多样本类别的样本权值**，其实这种做法等价于过采样和欠采样方法。

``【场景】``

### 四、单特征处理

单特征处理指的不仅仅是对单个特征的处理，也包含对多特征中的某一维特征的处理。

##### 4.1 归一化（Normalization）和标准化（Standardization）

对于XXX，【TODO】


所以需要采取归一化的方法，【TODO】


这两个概念由于长期的混用，到现在通常指的基本是同一个概念，就是无量纲化。但究其差别的话，**归一化内容稍微广泛一些，通常包含线性归一化**，比如

<center>
<img src="https://latex.codecogs.com/gif.latex?x'=\frac{x-min(x)}{max(x)-min(x)}"/>
</center>

还**包含非线性归一化**，比如 log(x)、exp(x)、tanh(x)。

而通常意义上的**标准化指代比较狭隘，指的是根据均值和标准差实现的线性放缩**，如下所示，

<center>
<img src="https://latex.codecogs.com/gif.latex?x'=\frac{x-\mu&space;}{\sigma&space;}"  />
</center>

应用场景 - 归一化【TODO】

``【场景】``

##### 4.2 二值化（定量特征离散化）

对于某些应用场景，我们更在乎是与否，而不关注程度，这种时候将定量特征的连续值转换成01值能提高学习效率。定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0（相反也可），公式表达如下：

<center><img src="https://latex.codecogs.com/gif.latex?x'=\left\{\begin{matrix}&space;1,&space;\&space;x>threshold\\&space;0,\&space;x\leq&space;threshold&space;\end{matrix}\right."/></center>

比如有以下场景，

``【场景-二值化】对于一个根据学生成绩来对表现进行分类的分类任务【分析】假如我们在音乐学习成绩这一项上通常只关心“及格”还是“不及格”，那么需要将定量的考分，转成“1”和“0”，分别表示及格和不及格，及格阈值是60分，那么分数大于等于60则转换成特征1，分数小于60则转换成特征0``

##### 4.3 虚拟变量（定性特征编码化）

虚拟变量，也叫哑变量、离散特征编码，可用来表示分类变量、非数据因素可能产生的影响。虚拟变量可划分成两种数据类型：

1. 离散特征的取值之间有大小的意义。例如：尺寸（L、XL、XXL）。这种情境很简单，直接用1，2，3来表示是合理的。
2. 离散特征的取值之间没有大小的意义。例如：颜色（Red、Blue、Green）。这种情况下，直接用1，2，3来表示存在一定的问题，因为模型的本质是函数，用1，2，3蕴含了额外的大小信息，比如2是在1和3之间，而实际上我们并不期望蓝色是介于红色和绿色之间，所以这样表示是不合理的。这个时候就需要借助热编码（one-hot encoding）。下图是《决战！平安京》对式神阵容进行热编码表征的示意图，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/one hot encoding.jpg"/>
</center>

```
【场景-虚拟编码】对于一种MOBA类游戏，我们要根据游戏世界状态进行游戏AI的操作训练【分析】英雄的BUFF类型和层数是影响操作的因素之一，比如《决战！平安京》中荒叠满5层星月之力就可以放出最大伤害的大招，那么在这个情境中，BUFF不同类型之间没有大小的意义，所以应采用类似于上图式神阵容那样的热编码形式表示；而BUFF不同层数之间有大小差别，所以可以直接用对应数字进行表示。
```

### 五、多特征处理

在实际操作时如果发现过拟合问题，除了考虑增加训练样本数，我们还应该考虑特征维度是否可以减少。有个东西叫curse of dimensionality，维度越高，数据在每个特征维度上的分布就越稀疏，这对机器学习算法基本都是灾难性的。

有人会说，特征数量过多，做前面的特征构建阶段砍掉不就行了。而事实上，很多问题的特征就是砍不掉。比如要研究某个罕见病跟什么基因有关，人类已知的基因有几千个，但是每个基因的真正作用目前技术并不能完全了解，能直接排除掉吗？并不能，这个时候就需要借助降维、特征选择的手段。事实上，在实际应用场景中如果特征维度过高，采用多特征处理方法，包括降维、特征选择，很有可能提高学习效果。

##### 5.1 降维

主成分分析（Principal Component Analysis，PCA）和线性判别分析（Linear Discriminant Analysis，LDA）是最常见和有效的降维方法，随着深度学习的兴起，使用深度学习的手段进行降维也十分有效。

**PCA**

相比于LDA，PCA还更常用一些，PCA优势在于它采用无监督（unsupervised）方式，不要求样本带类别信息，PCA的无监督优势很吸引人，因为自然界中任何信息都属于无类别信息，只有被人定义了类别的信息才是带类别信息，所以带类别信息相对于全体信息来说是十分稀有的。**由于任何多维特征都可以使用PCA处理，所以PCA更像是一个通用的预处理方法；PCA的主要作用是挖掘主成分和降维；PCA的本质思想是使主成分或者说降维后的特征每个维度之间方差最大**。以下是PCA的推导过程

1. 去除均值
2. 计算协方差矩阵
3. 计算协方差矩阵的特征值和特征向量
4. 将特征值排序
5. 保留前N大的特征值对应的特征向量作为新空间的基向量
6. 将数据转换到上面得到的N个特征向量构建的新空间中（实现了特征压缩）
 
 直接上代码
 
```
TODO
```

效果图

**LDA**

相比于PCA，LDA依赖类别信息，所以LDA没有PCA使用的那么广泛，但在有类别信息的情况下，LDA还可以作为一个独立的判别算法存在，给定了训练数据后，将会得到一系列的判别函数（discriminate function），之后对于新的输入，就可以进行预测了。

``【场景-LDA】``

**PCA和LDA的比较**

相同点：
（1）两者的作用是用来降维的
（2）两者都假设符合高斯分布

不同点
（1）LDA是有监督的降维方法，PCA是无监督的。
（2）LDA降维最多降到类别数K-1的维数，PCA没有这个限制。
（3）LDA更依赖均值，如果样本信息更依赖方差的话，效果将没有PCA好。
（4）LDA可能会过拟合数据。

同一批数据的比较 TODO

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/lda eg1.png"/>
</center>

**深度学习**

用深度学习的方法降维的结果表示的物理意义就没那么直观：

1. 无监督深度学习降维方法。让深度学习大热起来的那篇 Hinton 2006的文章，在分层预训练阶段，每一层都被看作一个玻尔兹曼机（Restricted Boltzmann Machine，RBM），并用统计概率上的马尔科夫随机场（Markov Random Fields）来解释，由多层 RBM 堆叠而成的网络被称为深度置信网络（Deep Belief Network，DBN）。RBM组成的DBN是一种无监督的降维方法。另一种同样大热无监督深度学习降维方法是堆叠自编码器（stack auto-encoder，SAE）
但是 大约从 2010 年开始 以 Hinton 的学生 Martens 为代表的一拨懂一点优化的人 开始放弃分层预训练繁琐的统计概率诠释 转而用全局优化的方法来训练 这种训练获得的每一层表示很难有直观的解释

2. 有监督深度学习降维方法。常见的有监督深度神经网络大类包括 多层感知机（Multiple-Layer Perception，MLP），卷积神经网络（Convolutional Neural Network，CNN），循环神经网络（Recurrent Neural Network，RNN）及其变种，各种深度神经网络的抽出某一层中间表示作为特征，都可以当做是降维后的特征。以CNN为例，每一个节点都是一个卷积过滤器，训练获得的系数就是每个过滤器的参数。

##### 5.2 特征选择

分为：过滤法（Filter）、封装法（Wrapper）以及 嵌入法（Embedded）。

**过滤法（Filter）**

过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

1. 移除低方差的特征 (Removing features with low variance)
2. 单变量特征选择 (Univariate feature selection)
+ 卡方(Chi2)检验
+ Pearson相关系数 (Pearson Correlation)
+ 互信息和最大信息系数 (Mutual information and maximal information coefficient (MIC)
2.4 距离相关系数 (Distance Correlation)

**封装式（Wrapper）**

包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。

递归特征消除（Recursive Feature Elimination）。递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，再基于新的特征集进行下一轮训练。

**嵌入式（Embedded）**

嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

1. 正则化

通过L1正则项来选择特征
L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，
但是要注意，L1没有选到的特征不代表不重要，原因是连个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验。
具体来说，应该分别使用L1和L2拟合，如果两个特征在L2中系数相接近，在L1中一个系数为0一个系数不为0，那么其实这两个特征都应该保留，原因是L1对于强相关特征只会保留一个。

2. 决策树（随机森林）或逻辑回归

训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型

[wiki: Normalization (statistics)](https://en.wikipedia.org/wiki/Normalization_(statistics))
[zhihu: 标准化和归一化什么区别？](https://www.zhihu.com/question/20467170)
[cnblogs: 使用sklearn做单机特征工程](http://www.cnblogs.com/jasonfreak/p/5448385.html)
[csdn: 机器学习算法在什么情况下需要归一化](https://blog.csdn.net/sinat_29508201/article/details/53056843)
[jianshu: 归一化、标准化和中心化/零均值化](https://www.jianshu.com/p/95a8f035c86c)
[jianshu: PCA与LDA比较](https://www.jianshu.com/p/982c8f6760de)
[csdn: 总结 特征选择（feature selection）算法笔记](https://blog.csdn.net/adore1993/article/details/53980327)
[cnblogs: 特征选择 (feature_selection)](https://www.cnblogs.com/stevenlk/p/6543628.html)