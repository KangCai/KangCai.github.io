---
layout: post
title: "机器学习 · 总览篇 XI"
subtitle: "特征工程"
author: "Kang Cai"
header-img: "img/post-bg-dreamer.jpg"
header-mask: 0.4
tags:
  - 机器学习
  - 机器学习·总览篇
---

> 前面的文章着重介绍了机器学习的三要素，包括模型、策略和算法，但

> 文章首发于[我的博客](https://kangcai.github.io/)，转载请保留链接 ;)

### 一、特征工程是什么

本文为了方便理解，在每种特征工程方法下都举个一个实例进行分析，基本可以表达出各个特征工程方法的思想，但不一定具备很强的代表性，仅起着抛砖引玉、引发思考的作用。

### 二、特征构建

通常有这么一种认知，认为机器学习会彻底占领人类专家擅长的领域，其实并不然，至少现阶段是错误的。特征构建作为机器学习流程的第一步，也是很重要的一步，是十分依赖专家知识的。

特征构建是指从原始数据中人工的找出一些具有物理意义的特征。需要花时间去观察原始数据，思考问题的潜在影响因素：

1. 对领域的认知、数据的敏感性、机器学习实战经验能有效地帮助我们进行特征构建，但这些都需要一定的积累和沉淀。
2. 除此之外，特征构建还有一些及时可以使用的技巧，**数据的分割和结合**：分割是指将某一种数据细化成多种数据，提供更多的信息，提高模型准确率；而结合是相反的过程，将冗余表达的信息合并在一起，提高模型收敛效率。

``【场景1-属性分割】根据大学生前两个学期的表现来预测他是否能在大学毕业时获得优秀毕业生的称号。【分析】直观的当然应该将学习成绩作为一个特征属性，但进一步进行考虑，通常专业必修课课成绩对评选影响更大，而非专业选修课影响会比较小，学习成绩又是本任务一个十分重要的影响因素，所以分开更合理，那么这种情况就是按学科将学习成绩属性分割。``


``【场景2-属性结合】预测大学生毕业后的薪资水平【分析】对于该任务，影响因素大概有学校、专业、学习成绩、获奖情况、课程项目经历、公司实习经历等，在这种情况下影响因素本身就较多，而且都相对比较重要。我们已有的数据包含了学生各科的成绩，如果直接拿来作为特征，根据《总览篇 VII 三要素之策略-正则化》可知，特征过多可能会导致过拟合；而过多太细化的特征实际影响并不大。所以，跟属性分割相反，将所有成绩做平均，或者最多将部分课程成绩结合分成专业课成绩和非专业课成绩是更合适的做法。``

可以看到，特征的构建是一个精准权衡的过程，需要做到信息 **“不冗余的完整”**，是个非常麻烦的问题，书里面也很少提到具体的方法，需要对问题有比较深入的理解。

### 三、特征清洗

##### 4.1 无效值和缺失值的处理

由于各种可能的因素，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法包括：

1. 估算（Estimation）。使用缺失数值所在行或列的均值、中位数、众数、最值等来替代缺失值。或者根据样本其它有相关性的数据来估算缺失数据。
2. 样本删除（Casewise Deletion）。当受到影响的样本占总样本比例较低，或者受影响的特征维度十分重要时，将有问题的样本删除是一种对学习效果影响较小的做法。
3. 变量删除（Variable Deletion）。当受到影响的样本较多，或者受影响的特征维度不太重要时，将有问题的样本删除是一种对学习效果影响较小的做法。有同学会说受影响的样本较多，受影响的特征维度又很重要怎么办？这个时候如果能重新收集数据最好，实在不行就需要在以上三种处理方法中权衡选择了。

``【场景3-无效值和缺失值的处理】根据人的体检指标对疾病自动进行诊断【分析】如果医院数据库的误操作将某一列全部删掉，或者某一位护士负责的那部分体检人的某列数据全部删掉。当缺失值是右眼视力，由于通常人两眼视力是接近的，所以可以用估算的方法，比如直接将左眼视力拷贝到右眼视力；当缺失值是少数体检人的部分重要数据，可以将样本剔除，再进行训练；当缺失值是身高，身高对疾病诊断的重要性不高，所以可以用变量删除的方法，将身高这一维度去除``

采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在数据收集中应当尽量避免出现无效值和缺失值，保证数据的完整性。

##### 4.2 采样

**样本均衡**

在分类中，训练数据不均衡是指不同类别下的样本数目相差巨大。比如对于一个二分类任务，如果训练集中类别1的样本数与类别2的样本数比值为100:1，使用逻辑回归进行分类，那么其结果很可能将所有样本都分类为类别1。样本不均衡会导致训练出的逻辑回归模型无效的原因在于，逻辑回归模型的代价函数是基于最大似然估计（MLE）的，某一类别的样本如果占绝大多数，会导致代价函数基本没有参考意义，因为模型当然会着重保证该类样本分类正确。

实际应用中，训练样本数据不均衡是常见并且合理的，比如在欺诈交易识别中，绝大部分交易是正常的，只有极少部分的交易属于欺诈交易；敏感头像（政治敏感、黄色相关的头像）检测中，绝大部分头像是正常的，只有极少部分头像属于政治敏感、黄色相关的。

通常有如下样本均衡方法：

1. 最直接的方法就是**扩充数据集，去收集更多的少样本类别的样本**；
2. 然后就是**拷贝过采样（over-sampling）**，即对少样本类别进行拷贝过采样来增加类别的样本量，以达到类别间样本均衡。
3. **欠采样（under-sampling）**，即对多样本类别进行欠采样来减少类别的样本量，以达到类别间样本均衡。
4. 人造数据，与拷贝过采样直接拷贝副本的方法不同，这里指的是构造全新样本的方法，代表性方法是SMOTE法，JAIR期刊2002年的文章[《SMOTE: Synthetic Minority Over-sampling Technique》](https://jair.org/index.php/jair/article/view/10302/24590)提出了一种过采样**算法SMOTE，概括来说，本算法基于“插值”来为少数类合成新的样本**。
5. 除此之外，对于boosting算法，它是采用多个弱学习器组合形成的强学习器，所以可以在**进行弱学习器训练时随机选取等量类别的样本**，当然，每个弱学习器都重新选择新的样本。

``【场景】``

**样本权重**

为了解决不同类别样本量差别过大的问题，样本均衡方法是从处理样本本身出发的，样本权重方法是从改变分类算法来实现的，**在代价函数中增加少样本类别的样本权值，降低多样本类别的样本权值**，其实这种做法等价于过采样和欠采样方法。

``【场景】``

### 四、单特征处理

单特征处理指的不仅仅是对单个特征的处理，也包含对多特征中的某一维特征的处理。

##### 4.1 归一化（Normalization）和标准化（Standardization）

对于XXX，【TODO】


所以需要采取归一化的方法，【TODO】


这两个概念由于长期的混用，到现在通常指的基本是同一个概念，就是无量纲化。但究其差别的话，**归一化内容稍微广泛一些，通常包含线性归一化**，比如

<center>
<img src="https://latex.codecogs.com/gif.latex?x'=\frac{x-min(x)}{max(x)-min(x)}"/>
</center>

还**包含非线性归一化**，比如 log(x)、exp(x)、tanh(x)。

而通常意义上的**标准化指代比较狭隘，指的是根据均值和标准差实现的线性放缩**，如下所示，

<center>
<img src="https://latex.codecogs.com/gif.latex?x'=\frac{x-\mu&space;}{\sigma&space;}"  />
</center>

应用场景 - 归一化【TODO】

``【场景】``

##### 4.2 二值化（定量特征离散化）

对于某些应用场景，我们更在乎是与否，而不关注程度，这种时候将定量特征的连续值转换成01值能提高学习效率。定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0（相反也可），公式表达如下：

<center><img src="https://latex.codecogs.com/gif.latex?x'=\left\{\begin{matrix}&space;1,&space;\&space;x>threshold\\&space;0,\&space;x\leq&space;threshold&space;\end{matrix}\right."/></center>

比如有以下场景，

``【场景-二值化】对于一个根据学生成绩来对表现进行分类的分类任务【分析】假如我们在音乐学习成绩这一项上通常只关心“及格”还是“不及格”，那么需要将定量的考分，转成“1”和“0”，分别表示及格和不及格，及格阈值是60分，那么分数大于等于60则转换成特征1，分数小于60则转换成特征0``

##### 4.3 虚拟变量（定性特征编码化）

虚拟变量，也叫哑变量、离散特征编码，可用来表示分类变量、非数据因素可能产生的影响。虚拟变量可划分成两种数据类型：

1. 离散特征的取值之间有大小的意义。例如：尺寸（L、XL、XXL）。这种情境很简单，直接用1，2，3来表示是合理的。
2. 离散特征的取值之间没有大小的意义。例如：颜色（Red、Blue、Green）。这种情况下，直接用1，2，3来表示存在一定的问题，因为模型的本质是函数，用1，2，3蕴含了额外的大小信息，比如2是在1和3之间，而实际上我们并不期望蓝色是介于红色和绿色之间，所以这样表示是不合理的。这个时候就需要借助热编码（one-hot encoding）。下图是《决战！平安京》对式神阵容进行热编码表征的示意图，

<center>
<img src="https://kangcai.github.io/img/in-post/post-ml/one hot encoding.jpg"/>
</center>

```
【场景-虚拟编码】对于一种MOBA类游戏，我们要根据游戏世界状态进行游戏AI的操作训练【分析】英雄的BUFF类型和层数是影响操作的因素之一，比如《决战！平安京》中荒叠满5层星月之力就可以放出最大伤害的大招，那么在这个情境中，BUFF不同类型之间没有大小的意义，所以应采用类似于上图式神阵容那样的热编码形式表示；而BUFF不同层数之间有大小差别，所以可以直接用对应数字进行表示。
```

### 五、多特征处理

在实际操作时如果发现过拟合问题，除了考虑增加训练样本数，我们还应该考虑特征维度是否可以减少。有个东西叫curse of dimensionality，维度越高，数据在每个特征维度上的分布就越稀疏，这对机器学习算法基本都是灾难性的。

有人会说，特征数量过多，做前面的特征构建阶段砍掉不就行了。而事实上，很多问题的特征就是砍不掉。比如要研究某个罕见病跟什么基因有关，人类已知的基因有几千个，但是每个基因的真正作用目前技术并不能完全了解，能直接排除掉吗？并不能，这个时候就需要借助降维、特征选择的手段。事实上，在实际应用场景中如果特征维度过高，采用多特征处理方法，包括降维、特征选择，很有可能提高学习效果。

##### 5.1 降维

主成分分析（Principal Component Analysis，PCA）和线性判别分析（Linear Discriminant Analysis，LDA）是最常见和有效的降维方法，随着深度学习的兴起，使用深度学习的手段进行降维也十分有效。

**PCA**

相比于LDA，PCA还更常用一些，PCA优势在于它采用无监督（unsupervised）方式，不要求样本带类别信息，PCA的无监督优势很吸引人，因为自然界中任何信息都属于无类别信息，只有被人定义了类别的信息才是带类别信息，所以带类别信息相对于全体信息来说是十分稀有的。**由于任何多维特征都可以使用PCA处理，所以PCA更像是一个通用的预处理方法；PCA的主要作用是挖掘主成分和降维；PCA的本质思想是使主成分或者说降维后的特征每个维度之间方差最大**。以下是PCA的推导过程

1. 去除均值
2. 计算协方差矩阵
3. 计算协方差矩阵的特征值和特征向量
4. 将特征值排序
5. 保留前N大的特征值对应的特征向量作为新空间的基向量
6. 将数据转换到上面得到的N个特征向量构建的新空间中（实现了特征压缩）
 
 直接上代码
 
```
TODO
```

效果图

**LDA**

LDA通常来说是作为一个独立的算法存在，给定了训练数据后，将会得到一系列的判别函数（discriminate function），之后对于新的输入，就可以进行预测了。

``【场景-LDA】``

**深度学习**

##### 5.2 特征选择

分为：过滤式（Filter）、封装式（Wrapper）以及 嵌入式（Embedded）。

**过滤式（Filter）**

**封装式（Wrapper）**

**嵌入式（Embedded）**

1. 正则化

2. 决策树

3. 深度学习

[wiki: Normalization (statistics)](https://en.wikipedia.org/wiki/Normalization_(statistics))
[zhihu: 标准化和归一化什么区别？](https://www.zhihu.com/question/20467170)
[cnblogs: 使用sklearn做单机特征工程](http://www.cnblogs.com/jasonfreak/p/5448385.html)
[csdn: 机器学习算法在什么情况下需要归一化](https://blog.csdn.net/sinat_29508201/article/details/53056843)
[jianshu: 归一化、标准化和中心化/零均值化](https://www.jianshu.com/p/95a8f035c86c)
[PCA与LDA比较](https://www.jianshu.com/p/982c8f6760de)